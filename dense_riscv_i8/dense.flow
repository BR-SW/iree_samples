module attributes {hal.device.targets = [#hal.device.target<"llvm-cpu", {executable_targets = [#hal.executable.target<"llvm-cpu", "embedded-elf-riscv_64", {cpu = "generic", cpu_features = "+m,+a,+d", data_layout = "e-m:e-p:64:64-i64:64-i128:128-n32:64-S128", native_vector_size = 16 : index, target_abi = "lp64d", target_triple = "riscv64-unknown-unknown-eabi-elf", ukernels = "default"}>]}>]} {
  flow.executable private @dense_dispatch_0 {
    flow.executable.export public @dense_dispatch_0_matmul_5x64x100_i8 workgroups() -> (index, index, index) {
      %c1 = arith.constant 1 : index
      flow.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module {
      func.func @dense_dispatch_0_matmul_5x64x100_i8(%arg0: !flow.dispatch.tensor<readonly:tensor<5x100xi8>>, %arg1: !flow.dispatch.tensor<readonly:tensor<100x64xi8>>, %arg2: !flow.dispatch.tensor<readonly:tensor<5x64xi8>>, %arg3: !flow.dispatch.tensor<readonly:tensor<5x64xi8>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<5x64xi8>>) {
        %c0_i8 = arith.constant 0 : i8
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [5, 100], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<5x100xi8>> -> tensor<5x100xi8>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [100, 64], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<100x64xi8>> -> tensor<100x64xi8>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [5, 64], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<5x64xi8>> -> tensor<5x64xi8>
        %3 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [5, 64], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<5x64xi8>> -> tensor<5x64xi8>
        %4 = tensor.empty() : tensor<5x64xi8>
        %5 = linalg.fill ins(%c0_i8 : i8) outs(%4 : tensor<5x64xi8>) -> tensor<5x64xi8>
        %6 = linalg.matmul ins(%0, %1 : tensor<5x100xi8>, tensor<100x64xi8>) outs(%5 : tensor<5x64xi8>) -> tensor<5x64xi8>
        %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%6, %2, %3 : tensor<5x64xi8>, tensor<5x64xi8>, tensor<5x64xi8>) outs(%4 : tensor<5x64xi8>) {
        ^bb0(%in: i8, %in_0: i8, %in_1: i8, %out: i8):
          %8 = arith.addi %in, %in_0 : i8
          %9 = arith.minsi %8, %in_1 : i8
          linalg.yield %9 : i8
        } -> tensor<5x64xi8>
        flow.dispatch.tensor.store %7, %arg4, offsets = [0, 0], sizes = [5, 64], strides = [1, 1] : tensor<5x64xi8> -> !flow.dispatch.tensor<writeonly:tensor<5x64xi8>>
        return
      }
    }
  }
  flow.executable private @dense_dispatch_1 {
    flow.executable.export public @dense_dispatch_1_matmul_5x10x64_i8 workgroups() -> (index, index, index) {
      %c1 = arith.constant 1 : index
      flow.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module {
      func.func @dense_dispatch_1_matmul_5x10x64_i8(%arg0: !flow.dispatch.tensor<readonly:tensor<5x64xi8>>, %arg1: !flow.dispatch.tensor<readonly:tensor<64x10xi8>>, %arg2: !flow.dispatch.tensor<readonly:tensor<5x10xi8>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<5x10xi8>>) {
        %c0_i8 = arith.constant 0 : i8
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [5, 64], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<5x64xi8>> -> tensor<5x64xi8>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [64, 10], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<64x10xi8>> -> tensor<64x10xi8>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [5, 10], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<5x10xi8>> -> tensor<5x10xi8>
        %3 = tensor.empty() : tensor<5x10xi8>
        %4 = linalg.fill ins(%c0_i8 : i8) outs(%3 : tensor<5x10xi8>) -> tensor<5x10xi8>
        %5 = linalg.matmul ins(%0, %1 : tensor<5x64xi8>, tensor<64x10xi8>) outs(%4 : tensor<5x10xi8>) -> tensor<5x10xi8>
        %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<5x10xi8>, tensor<5x10xi8>) outs(%3 : tensor<5x10xi8>) {
        ^bb0(%in: i8, %in_0: i8, %out: i8):
          %7 = arith.addi %in, %in_0 : i8
          linalg.yield %7 : i8
        } -> tensor<5x10xi8>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [5, 10], strides = [1, 1] : tensor<5x10xi8> -> !flow.dispatch.tensor<writeonly:tensor<5x10xi8>>
        return
      }
    }
  }
  func.func @dense(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.buffer_view, %arg5: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @dense(%input0: tensor<5x100xi8>, %input1: tensor<100x64xi8>, %input2: tensor<5x64xi8>, %input3: tensor<5x64xi8>, %input4: tensor<64x10xi8>, %input5: tensor<5x10xi8>) -> (%output0: tensor<5x10xi8>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<5x100xi8>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<100x64xi8>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<5x64xi8>
    %3 = hal.tensor.import %arg3 "input3" : !hal.buffer_view -> tensor<5x64xi8>
    %4 = hal.tensor.import %arg4 "input4" : !hal.buffer_view -> tensor<64x10xi8>
    %5 = hal.tensor.import %arg5 "input5" : !hal.buffer_view -> tensor<5x10xi8>
    %6 = flow.dispatch @dense_dispatch_0::@dense_dispatch_0_matmul_5x64x100_i8(%0, %1, %2, %3) : (tensor<5x100xi8>, tensor<100x64xi8>, tensor<5x64xi8>, tensor<5x64xi8>) -> tensor<5x64xi8>
    %7 = flow.dispatch @dense_dispatch_1::@dense_dispatch_1_matmul_5x10x64_i8(%6, %4, %5) : (tensor<5x64xi8>, tensor<64x10xi8>, tensor<5x10xi8>) -> tensor<5x10xi8>
    %8 = hal.tensor.export %7 "output0" : tensor<5x10xi8> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}
